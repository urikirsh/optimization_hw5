\documentclass[12pt]{scrartcl}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{float}
\usepackage{bm}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}

\usepackage{csvsimple}

\begin{document}

%Title section

\titlehead{CS department, Technion}
\subject{Introduction to Optimization and Deep Learning 236330}
\title{HW 5}
\subtitle{Training Neural Networks with SGD and Adagrad Methods}
\author{Uri Kirstein 311137095 \hfill sukirstn@campus.technion.ac.il\and Pavel Rastopchin 321082026 pavelr@campus.technion.ac.il}
\date{\today}
\maketitle

%end of title section

\section{Finding optimal learning rate}
\subsection{Log scale search}
In order to optimize the initial training rate (noted below as Alpha 0) and decay constant k, we ran the network on different values of those parameters in a log scale search. The learning rate scale runs from 0.1 to $10^-6$, while the decay constant runs from 1 (no decay) to $10^-7$.\\

We tested the different values on both the Stochastic Gradient Descent and Adagrad algorithms. In all the tests below we have used the same weights, which were generated vie the Xavier initialization method. In all tests below batch size was 32.\\
Each test was run on a train set that contains 500 points, like last exercise. We ran the optimization algorithms for 1000 epochs each. After that we used the obtained weights and biases, fed them to the network and checked the loss on the test set. The test set consists of 200 points. The training and test sets were created in an identical way.

\csvautotabular{SGD_TR_losses.csv}

\csvautotabular{Adagrad_TR_losses.csv}

Both algorithms have very similar results for each test. Adagrad is slightly better, but at an insignificant margin. The best (smallest) test loss is achieved for both algorithms with $\alpha_0 = 0.01$ and a decay constant of $k=0.001$. In SGD the minimal loss is 0.017828073039011887 and in Adagrad it is 0.017895997004666164 - less than a 1\% apart. Both minima are $\sim 1.6\%$ smaller than the second smallest test loss.\\

The train loss is unsurprisingly lower for all parameter values. In many instances it is lower by several orders of magnitude, but not always. The minimum train loss is achieved at $\alpha_0=0.1$ and $k=10^{-7}$ for both algorithms. The minimal training loss is 7.9176597493986e-34 with SGD and 9.086101134115954e-34 with Adagrad. It is the smallest by a significant margin. The minimal test loss is 32 orders of magnitude greater than that number. However, if we look at the test loss achieved in $\alpha_0=0.1$ and $k=10^{-7}$ we see that it is ~2.2\% larger than the minimal test loss. As the test loss is the metric we truly care about, we will continue with the parameters that minimize it.\\

We will now concentrate our effort at the parameter values where $0.001 \leq \alpha_0 \leq 0.1$ and $10^{-4} \leq k \leq 0.01$.

\subsection{Linear Search}
The tests on this part were conducted on a fresh set of parameters. We checked the following values of parameters:
\begin{itemize}
\item $\alpha_0$ = 0.001, 0.002, 0.003 ... 0.009, 0.01, 0.02, 0.03 ... 0.1
\item k = $10^{-4}$, $2 \cdot 10^{-4}$, $3 \cdot 10^{-4}$ ... $9 \cdot 10^{-4}$, $10^{-3}$, $2 \cdot 10^{-3}$, $3 \cdot 10^{-3}$ ... $10^{-2}$
\end{itemize}

Because 400 tests were conducted, the results are attached in a separate csv file.\\\\
\textbf{SGD results:}
\begin{itemize}
\item \textit{Minimal test loss} - Achieved for $\alpha_0 = 0.03$ and $k=10^{-4}$. The minimal test loss is 0.018975106.
\item \textit{Minimal train loss} - Achieved for $\alpha_0 = 0.04$ and $k=10^{-4}$. The minimal train loss is 1.47E-19
. Notice it shares the same k and has a very close $\alpha_0$ to the minimal test loss point.
\end{itemize}

\textbf{Adagrad results:}
\begin{itemize}
\item \textit{Minimal test loss} - Achieved for $\alpha_0 = 0.1$ and $k=8\cdot 10^{-4}$. The minimal test loss is 0.019047919. This is very close to the minimal test loss of SGD.
\item \textit{Minimal train loss} - Achieved for $\alpha_0 = 0.1$ and $k=10^{-4}$. The minimal train loss is 1.18407E-15. This is 4 orders of magnitude larger than the minimal tran loss of SGD.
\end{itemize}
Because the the two losses in the SGD algorithm are highly linked suggesting a more stable result, and because both losses are smaller in the SGD algorithm, we will proceed with the optimal parameters of the SGD algorithm in terms of test loss.

\section{Finding optimal batch size}
Batch sizes are usually chosen to be powers of 2. Usually the minimum batch size considered is 32. We decided to do a more exhaustive search and start from 16. We also test what happens if the batch size is our entire set - 500 points.\\
All those tests were run with a different parameter initialization than tests on previous parts.

\csvautotabular{Batch_sizes.csv}\\\\

The first thing we note is that all test losses are incredibly close - the batch size has little to no effect on the training loss, in both algorithms.\\
For SGD, the minimal test loss is 0.026045487, for a batch size of 16. The minimal training loss is 9.40E-36 for a batch size of 64.\\
For Adagrad, the minimal test loss is 0.026042683, for a batch size of 16. The minimal training loss is 3.50E-27 for a batch size of 256.\\
Overall we see that the minimal test loss is nearly identical in both algorithms, and is achieved with the same batch size of 16. This also happens to be the smallest, therefore quickest batch size to compute.\\
We can also notice that while the test losses are incredibly close, the train losses are far smaller in SGD. This could have been a red flag for overfitting, if not for the identical results in test loss between the two algorithms.\\
\textbf{Conclusion - the chosen batch size is 16.}

\section{Shuffling the data}

\section{References}
\begin{itemize}
\item Log scale search - \url{https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/}

\item Log scale search - \url{https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1}

\item Finding optimal batch size - \url{https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network}
\end{itemize}

\end{document}

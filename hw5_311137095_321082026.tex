\documentclass[12pt]{scrartcl}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{float}
\usepackage{bm}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}

\usepackage{csvsimple}

\begin{document}

%Title section

\titlehead{CS department, Technion}
\subject{Introduction to Optimization and Deep Learning 236330}
\title{HW 5}
\subtitle{Training Neural Networks with SGD and Adagrad Methods}
\author{Uri Kirstein 311137095 \hfill sukirstn@campus.technion.ac.il\and Pavel Rastopchin 321082026 pavelr@campus.technion.ac.il}
\date{\today}
\maketitle

%end of title section

\section{Finding optimal learning rate}
\subsection{Log scale search}
In order to optimize the initial training rate (noted below as Alpha 0) and decay constant k, we ran the network on different values of those parameters in a log scale search. The learning rate scale runs from 0.1 to $10^-6$, while the decay constant runs from 1 (no decay) to $10^-7$. We used the following sources as a reference:\\
\url{https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/}\\
\url{https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1}\\
We tested the different values on both the Stochastic Gradient Descent and Adagrad algorithms. In all the tests below we have used the same weights, which were generated vie the Xavier initialization method. In all tests below batch size was 32.\\
Each test was run on a train set that contains 500 points, like last exercise. We ran the optimization algorithms for 1000 epochs each. After that we used the obtained weights and biases, fed them to the network and checked the loss on the test set. The test set consists of 200 points. The training and test sets were created in an identical way.

\csvautotabular{SGD_TR_losses.csv}

\csvautotabular{Adagrad_TR_losses.csv}

Both algorithms have very similar results for each test. Adagrad is slightly better, but at an insignificant margin. The best (smallest) test loss is achieved for both algorithms with $\alpha_0 = 0.01$ and a decay constant of $k=0.001$. In SGD the minimal loss is 0.017828073039011887 and in Adagrad it is 0.017895997004666164 - less than a 1\% apart. Both minima are $\sim 1.6\%$ smaller than the second smallest test loss.\\

The train loss is unsurprisingly lower for all parameter values. In many instances it is lower by several orders of magnitude, but not always. The minimum train loss is achieved at $\alpha_0=0.1$ and $k=10^{-7}$ for both algorithms. The minimal training loss is 7.9176597493986e-34 with SGD and 9.086101134115954e-34 with Adagrad. It is the smallest by a significant margin. The minimal test loss is 32 orders of magnitude greater than that number. However, if we look at the test loss achieved in $\alpha_0=0.1$ and $k=10^{-7}$ we see that it is ~2.2\% larger than the minimal test loss. As the test loss is the metric we truly care about, we will continue with the parameters that minimize it.\\

We will now concentrate our effort at the parameter values where $0.001 \leq \alpha_0 \leq 0.1$ and $10^{-4} \leq k \leq 0.01$.

\subsection{Linear Search}
The tests on this part were conducted on a fresh set of parameters. We checked the following values of parameters:
\begin{itemize}
\item $\alpha_0$ = 0.001, 0.002, 0.003 ... 0.009, 0.01, 0.02, 0.03 ... 0.1
\item k = $10^{-4}$, $2 \cdot 10^{-4}$, $3 \cdot 10^{-4}$ ... $9 \cdot 10^{-4}$, $10^{-3}$, $2 \cdot 10^{-3}$, $3 \cdot 10^{-3}$ ... $10^{-2}$
\end{itemize}

Because 400 tests were conducted, the results are attached in a separate csv file.\\\\
\textbf{SGD results:}
\begin{itemize}
\item \textit{Minimal test loss} - Achieved for $\alpha_0 = 0.03$ and $k=10^{-4}$. The minimal test loss is 0.018975106.
\item \textit{Minimal train loss} - Achieved for $\alpha_0 = 0.04$ and $k=10^{-4}$. The minimal train loss is 1.47E-19
. Notice it shares the same k and has a very close $\alpha_0$ to the minimal test loss point.
\end{itemize}

\textbf{Adagrad results:}
\begin{itemize}
\item \textit{Minimal test loss} - Achieved for $\alpha_0 = 0.1$ and $k=8\cdot 10^{-4}$. The minimal test loss is 0.019047919. This is very close to the minimal test loss of SGD.
\item \textit{Minimal train loss} - Achieved for $\alpha_0 = 0.1$ and $k=10^{-4}$. The minimal train loss is 1.18407E-15. This is 4 orders of magnitude larger than the minimal tran loss of SGD.
\end{itemize}
Because the the two losses in the SGD algorithm are highly linked suggesting a more stable result, and because both losses are smaller in the SGD algorithm, we will proceed with the optimal parameters of the SGD algorithm.

\section{Finding optimal batch size}

\section{Shuffling the data}

\end{document}

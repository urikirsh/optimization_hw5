\documentclass[12pt]{scrartcl}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{float}
\usepackage{bm}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}

\usepackage{csvsimple}

\begin{document}

%Title section

\titlehead{CS department, Technion}
\subject{Introduction to Optimization and Deep Learning 236330}
\title{HW 5}
\subtitle{Training Neural Networks with SGD and Adagrad Methods}
\author{Uri Kirstein 311137095 \hfill sukirstn@campus.technion.ac.il\and Pavel Rastopchin 321082026 pavelr@campus.technion.ac.il}
\date{\today}
\maketitle

%end of title section

\section{Finding optimal learning rate}
\subsection{Log scale search}
In order to optimize the initial training rate (noted below as Alpha 0) and decay constant k, we ran the network on different values of those parameters in a log scale search. The learning rate scale runs from 0.1 to $10^-6$, while the decay constant runs from 1 (no decay) to $10^-7$.\\

We tested the different values on both the Stochastic Gradient Descent and Adagrad algorithms. In all the tests below we have used the same weights, which were generated vie the Xavier initialization method. In all tests below batch size was 32.\\
Each test was run on a train set that contains 500 points, like last exercise. We ran the optimization algorithms for 1000 epochs each. After that we used the obtained weights and biases, fed them to the network and checked the loss on the test set. The test set consists of 200 points. The training and test sets were created in an identical way.

\csvautotabular{SGD_TR_losses.csv}

\csvautotabular{Adagrad_TR_losses.csv}

Both algorithms have very similar results for every test loss. Adagrad is slightly better, but at an insignificant margin. The best (smallest) test loss is achieved for both algorithms with $\alpha_0 = 0.1$ and a decay constant of $k=10^{-5}$. In SGD the minimal test loss is 0.012546461 and in Adagrad it is 0.017895997004666164 - less than a 1\% apart. Both minima are $\sim 1.6\%$ smaller than the second smallest test loss.\\

The train loss is unsurprisingly lower for most parameter values. In many instances it is lower by several orders of magnitude, but not always. The minimum train loss is achieved at $\alpha_0=0.1$ and $k=10^{-6}$ for SGD and is 0.008323657. With Adagrad the minimal train loss is 0.010536475 and is achieved when $\alpha_0 = 0.01$ and $k=0.001$. As the test loss is the metric we truly care about, we will continue with the parameters that minimize it.\\

We note that the maximum is in an almost flat area in regards to the decay rate. We will now concentrate our effort at the parameter values where $0.01 \leq \alpha_0 \leq 0.5$ and $k = 10^{-6}$.

\subsection{Linear Search}
The tests on this part were conducted on a fresh set of parameters. We checked the following values of parameters:
\begin{itemize}
\item $\alpha_0$ = 0.01, 0.02, 0.03 ... 0.1, 0.2, 0.3 ... 0.5
\item k = $10^{-6}$
\end{itemize}

\csvautotabular{SGD_fine_TR_losses.csv}

\csvautotabular{Adagrad_fine_TR_losses.csv}\\\\

\textbf{SGD results:}
\begin{itemize}
\item \textit{Minimal test loss} - Achieved for $\alpha_0 = 0.3$. The minimal test loss is 0.002861623.
\item \textit{Minimal train loss} - Achieved for $\alpha_0 = 0.3$ as well. The minimal train loss is 0.002386384.
\end{itemize}

\textbf{Adagrad results:}
\begin{itemize}
\item \textit{Minimal test loss} - Achieved for $\alpha_0 = 0.5$. The minimal test loss is 0.003210535.
\item \textit{Minimal train loss} - Achieved for $\alpha_0 = 0.4$. The minimal train loss is 0.003427235. Note that the two maxima are close to each other in the $\alpha_0$ parameter.
\end{itemize}
Here we see that SGD gets smaller test losses than Adagrad consistently. The margin however is not big and the two algorithm exhibit similar behaviour - their losses increase and decrease together.\\
We chose to continue with the minima of SGD for two main reasons. First, in SGD the train loss and test loss minima coincide, indicating a more stable result. Second, the losses in the SGD function are lower, meaning it is likely to be the algorithm of choice so it makes more sense to maximize it.

\section{Finding optimal batch size}
Batch sizes are usually chosen to be powers of 2. Usually the minimum batch size considered is 32. We decided to do a more exhaustive search and start from 16. We also test what happens if the batch size is our entire set - 500 points.\\
All those tests were run with a different parameter initialization than tests on previous parts.

\csvautotabular{Batch_sizes.csv}\\\\

The first thing we note is that all test losses are incredibly close - the batch size has little to no effect on the training loss, in both algorithms.\\
For SGD, the minimal test loss is 0.026045487, for a batch size of 16. The minimal training loss is 9.40E-36 for a batch size of 64.\\
For Adagrad, the minimal test loss is 0.026042683, for a batch size of 16. The minimal training loss is 3.50E-27 for a batch size of 256.\\
Overall we see that the minimal test loss is nearly identical in both algorithms, and is achieved with the same batch size of 16. This also happens to be the smallest, therefore quickest batch size to compute.\\
We can also notice that while the test losses are incredibly close, the train losses are far smaller in SGD. This could have been a red flag for overfitting, if not for the identical results in test loss between the two algorithms.\\
\textbf{Conclusion - the chosen batch size is 16.}

\section{Shuffling the data}
HERE WE WILL DISCUSS

\section{References}
\begin{itemize}
\item Log scale search - \url{https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/}

\item Log scale search - \url{https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1}

\item Finding optimal batch size - \url{https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network}
\end{itemize}

\end{document}
